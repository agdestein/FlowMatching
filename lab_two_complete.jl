using ForwardDiff: vector_mode_jacobian
# %% [markdown]
# # Lab Two: Flow Matching and Score Matching

# %% [markdown]
# Welcome to lab two! In this lab, we will provide an intuitive and hands-on walk-through of *flow matching* and *score matching*. If you find any mistakes, or have any other feedback, please feel free to email us at `erives@mit.edu` and `phold@mit.edu`. Enjoy!

# %% [markdown]
# ### Part 0: Miscellaneous Imports and Utility Functions

using Adapt
using Distributions
using ForwardDiff
using LinearAlgebra
using Lux
using NNlib
using Optimisers
using Random
using WGLMakie
using Zygote

function simulate(step, x, ts)
    for t_idx in eachindex(ts)
        t = ts[t_idx]
        h = ts[t_idx+1] - ts[t_idx]
        x = step(x, t, h)
    end
    x
end

function simulate_with_trajectory(step, x, ts)
    xs = [x]
    for t_idx = 1:length(ts)-1
        t = ts[t_idx]
        h = ts[t_idx+1] - ts[t_idx]
        x = step(x, t, h)
        push!(xs, x)
    end
    stack(xs)
end

# %%
euler_step(drift, xt, t, h) = xt + drift(xt, t) * h

# %%
function euler_maruyama_step(drift, diff, xt, t, h)
    a = drift(xt, t)
    b = diff(xt, t)
    @. xt + a * h + b * sqrt(h) * randn()
end

score(density, x) = ForwardDiff.gradient(x -> logpdf(density, x), x)

function random_2D(nmodel, std, scale = 10.0)
    dim = 2
    models = map(1:nmodel) do i
        r = rand(dim)
        mean = @. (r - 0.5) * scale
        cov = std^2 * I(dim)
        MvNormal(mean, cov)
    end
    MixtureModel(models)
end

function symmetric_2D(n, std, scale = 10.0)
    angles = range(0, 2π, n + 1)[2:end]
    dim = 2
    models = map(angles) do θ
        mean = scale * [cos(θ), sin(θ)]
        cov = Diagonal(fill(std^2, dim))
        MvNormal(mean, cov)
    end
    MixtureModel(models)
end

isotropic(std) = MvNormal(zeros(2), std^2 * I(2))

# %% [markdown]
# ### Part 1: Implementing Conditional Probability Paths
# Recall from lecture and the class notes the basic premise of conditional flow matching: describe a *conditional probability path* $p_t(x|z)$, so that $p_1(x|z) = \delta_z(x)$, and $p_0(z) = p_{\text{simple}}$ (e.g., a Gaussian), and $p_t(x|z)$ interpolates continuously (we are not being rigorous here) between $p_0(x|z)$ and $p_1(x|z)$. Such a conditional path can be seen as corresponding to some corruption process which (in reverse time) drives the point $z$ at $t=1$ to be distribution as $p_0(x|z)$ at time $t=0$. Such a corruption process is given by the ODE
# $$dX_t = u_t^{\text{ref}}(X_t|z)\,dt,\quad \quad X_0 \sim p_{\text{simple}}.$$
# The drift $u_t^{\text{ref}}(X_t|z)$ is referred to as the *conditional vector field*. By averaging $u_t^{\text{ref}}(x|z)$ over all such choices of $z$, we obtain the *marginal* vector field $u_t^{\text{ref}}(x)$. Flow matching proposes to exploit the fact that the *marginal probability path* $p_t(x)$ generated by the marginal vector field $u_t^{\text{ref}}(x)$, bridges $p_{\text{simple}}$ to $p_{\text{data}}$. Since the conditional vector field $u_t^{\text{ref}}(x|z)$ is often analytically available, we may implicitly regress against the unknown marginal vector field $u_t^{\text{ref}}(x)$ by explicitly regressing against the conditional vector field $u_t^{\text{ref}}(x|z)$.

# %% [markdown]
# The central object in this construction is a *conditional probability path*, whose interface is implemented below in the class `ConditionalProbabilityPath`. In this lab, you will implement two subclasses: `GaussianConditionalProbabilityPath`, and `LinearConditionalProbabilityPath` corresponding to probability paths of the same names from the lectures and notes.

# %% [markdown]
# # Part 2: Gaussian Conditional Probability Paths
# In this section, we'll implement a **Gaussian conditional probability path** via the class `GaussianConditionalProbabilityPath`. We will then use it to transform a simple source $p_{\text{simple}} = N(0, I_d)$ into a Gaussian mixture $p_{\text{data}}$. Later, we'll experiment with more exciting distributions. Recall that a Gaussian conditional probability path is given by
# $$p_t(x|z) = N(x;\alpha_t z,\beta_t^2 I_d),\quad\quad\quad p_{\text{simple}}=N(0,I_d),$$
# where $\alpha_t: [0,1] \to \mathbb{R}$ and $\beta_t: [0,1] \to \mathbb{R}$ are monotonic, continuously differentiable functions satisfying $\alpha_1 = \beta_0 = 1$ and $\alpha_0 = \beta_1 = 0$. In other words, this implies that $p_1(x|z) = \delta_z$ and $p_0(x|z) = N(0, I_d)$ is a unit Gaussian. Before we dive into things, let's take a look at $p_{\text{simple}}$ and $p_{\text{data}}$.

# %%
# Constants for the duration of our use of Gaussian conditional probability paths, to avoid polluting the namespace...
PARAMS = (; scale = 15.0, target_scale = 10.0, target_std = 1.0)

# %%
p_simple = isotropic(; dim = 2, std = 1.0)
p_data = symmetric_2D(5, PARAMS.target_std, PARAMS.target_scale)

function densityheatmap!(ax; scale, nbin, distribution, dolog = true, kwargs...)
    x = range(-scale, scale, nbin)
    y = reshape(x, 1, :)
    p = broadcast((x, y) -> pdf(distribution, [x, y]), x, y)
    m = maximum(p)
    heatmap!(
        ax,
        x,
        x,
        p;
        colorscale = dolog ? log10 : identity,
        colorrange = (1e-8, m),
        colormap = :Blues,
        kwargs...,
    )
end

let
    nbin = 200
    fig = Figure()
    scale = PARAMS.scale
    bounds = -scale, scale
    ax = Axis(fig[1, 1]; title = "Simple distribution", aspect = DataAspect())
    densityheatmap!(ax; scale, nbin, distribution = p_simple)
    ax = Axis(fig[1, 2]; title = "Data distribution", aspect = DataAspect())
    densityheatmap!(ax; scale, nbin, distribution = p_data)
    ax = Axis(fig[1, 3]; title = "Both distributions", aspect = DataAspect())
    densityheatmap!(ax; scale, nbin, distribution = p_simple)
    densityheatmap!(
        ax;
        scale,
        nbin,
        distribution = p_data,
        colormap = :reds,
        lowclip = RGBAf(0.0, 0.0, 0.0, 0.0),
    )
    fig
end

# %% [markdown]
# ### Problem 2.1: Implementing $\alpha_t$ and $\beta_t$

# %% [markdown]
# Let's get started by implementing $\alpha_t$ and $\beta_t$. We can think of these simply as callable objects which fulfill the simple contract $\alpha_1 = \beta_0 = 1$ and $\alpha_0 = \beta_1 = 0$, and which can compute their time derivatives $\dot{\alpha}_t$ and $\dot{\beta}_t$. We implement them below via the classes `Alpha` and `Beta`.

# %% [markdown]
# In this section, we'll be using $$\alpha_t = t \quad \quad \text{and} \quad \quad \beta_t = \sqrt{1-t}.$$ It is not hard to check that both functions are continuously differentiable on $[0,1)$, and monotonic, that $\alpha_1 = \beta_0 = 1$, and that $\alpha_0 = \beta_1 = 0$.
#
# **Your job**: Implement the `__call__` methods of the classes `LinearAlpha` and `SquareRootBeta` below.

# %% [markdown]
# Let us know turn towards the task of implementing the `GaussianConditionalProbabilityPath` path.

# %% [markdown]
# ### Problem 2.2: Gaussian Conditional Probability Path

# %% [markdown]
# **Your work**: Implement the class method `sample_conditional_path` to sample from the conditional distribution $p_t(x|z) = N(x;\alpha_t z,\beta_t^2 I_d)$. You can check the correctness of your implementation by running the next two cells to generate an image of the conditional probability path and comparing these to the corresponding plot from Figure 6 in the lecture notes (the one labeled "Ground-Truth Conditional Probability Path").
#
# **Hint**: You may use the fact that the random variable $X \sim N(\mu, \sigma^2 I_d)$ is obtained via $X = \mu + \sigma Z$, where $Z \sim N(0, I_d)$.

# %% [markdown]
# We can now sample from, and thus visualize, the *conditional* probaability path.

# %%
# Construct conditional probability path
alpha(t) = t
beta(t) = sqrt(1 - t)

let
    p_data = symmetric_2D(5, PARAMS.target_std, PARAMS.target_scale)
    z = rand(p_data) # Sample conditioning variable z
    ts = range(0.0, 1.0, 7)
    nbin = 200
    scale = PARAMS.scale
    fig = Figure()
    ax = Axis(
        fig[1, 1];
        title = "Gaussian Conditional Probability Path",
        aspect = DataAspect(),
    )
    # Plot source and target
    dolog = true
    densityheatmap!(ax; scale, nbin, distribution = p_simple, dolog)
    densityheatmap!(
        ax;
        scale,
        nbin,
        distribution = p_data,
        colormap = :reds,
        lowclip = RGBAf(0.0, 0.0, 0.0, 0.0),
        dolog,
    )
    scatter!(ax, z...; marker = :star5, markersize = 20, color = :black)
    # Plot conditional probability path at each intermediate t
    num_samples = 1000
    x0 = rand(p_simple, num_samples) # Sample from p_simple
    for t in ts
        a, b = alpha(t), beta(t)
        samples = @. a * z + b * x0
        scatter!(
            ax,
            samples[1, :],
            samples[2, :];
            alpha = 0.25,
            markersize = 8,
            label = "t = $(round(t; sigdigits = 1))",
        )
    end
    Legend(fig[1, 2], ax; tellwidth = true)
    fig
end

# %% [markdown]
# ### Problem 2.3: Conditional Vector Field
# From lecture and the notes, we know that the conditional vector field $u_t(x|z)$ is given by
# $$u_t(x|z) = \left(\dot{\alpha}_t-\frac{\dot{\beta}_t}{\beta_t}\alpha_t\right)z+\frac{\dot{\beta}_t}{\beta_t}x.$$

# %% [markdown]
# **Your work**: Implement the class method `conditional_vector_field` to compute the conditional vector field $u_t(x|z)$.
#
# **Hint**: You can compute $\dot{\alpha}_t$ with `self.alpha.dt(t)`, which has been implemented for you. You may compute $\dot{\beta}_t$ similarly.

# %% [markdown]
# We may now visualize the conditional trajectories corresponding to the ODE $$d X_t = u_t(X_t|z)dt, \quad \quad X_0 = x_0 \sim p_{\text{simple}}.$$

# %%
# Run me for Problem 2.3!

########################
# Setup path and plot  #
########################

function gaussian_conditional_vector_field(x, z, t)
    alpha_t = alpha(t)
    beta_t = beta(t)
    dt_alpha_t = ForwardDiff.derivative(alpha, t)
    dt_beta_t = ForwardDiff.derivative(beta, t)
    (dt_alpha_t - dt_beta_t / beta_t * alpha_t) * z + dt_beta_t / beta_t * x
end

let
    #######################
    # Change these values #
    #######################
    num_samples = 1000
    num_timesteps = 1000
    num_marginals = 3
    #
    p_data = symmetric_2D(5, PARAMS.target_std, PARAMS.target_scale)
    z = rand(p_data) # Sample conditioning variable z
    ts = range(0.0, 1.0, 7)
    nbin = 200
    scale = PARAMS.scale
    fig = Figure()
    ax = Axis(
        fig[1, 1];
        title = "Gaussian Conditional Probability Path",
        aspect = DataAspect(),
    )
    densityheatmap!(ax; scale, nbin, distribution = p_simple)
    densityheatmap!(
        ax;
        scale,
        nbin,
        distribution = p_data,
        colormap = :reds,
        lowclip = RGBAf(0.0, 0.0, 0.0, 0.0),
    )
    # Plot conditional probability path at each intermediate t
    num_samples = 1000
    x0 = rand(p_simple, num_samples) # Sample from p_simple
    for t in ts
        a, b = alpha(t), beta(t)
        samples = @. a * z + b * x0
        scatter!(
            ax,
            samples[1, :],
            samples[2, :];
            alpha = 0.25,
            markersize = 8,
            label = "t = $(round(t; sigdigits = 1))",
        )
    end
    scatter!(ax, z...; marker = :star5, markersize = 20, color = :green)
    axislegend(ax)
    ax = Axis(fig[1, 2]; title = "Trajectories of conditional ODE", aspect = DataAspect())
    densityheatmap!(ax; scale, nbin, distribution = p_simple)
    densityheatmap!(
        ax;
        scale,
        nbin,
        distribution = p_data,
        colormap = :reds,
        lowclip = RGBAf(0.0, 0.0, 0.0, 0.0),
    )
    for i = 1:15
        x0 = rand(p_simple)
        t = range(0.0, 1.0, num_timesteps)
        drift = (x, t) -> gaussian_conditional_vector_field(x, z, t)
        step = (x, t, h) -> euler_step(drift, x, t, h)
        x = simulate_with_trajectory(step, x0, t)
        lines!(ax, x[1, :], x[2, :]; color = :black)
    end
    scatter!(ax, z...; marker = :star5, markersize = 20, color = :green)
    fig
end

# %% [markdown]
# **Note**: You may have noticed that since for Gaussian probability paths, $z \sim p_{\text{data}}(x)$, the method `GaussianConditionalProbabilityPath.sample_conditioning_variable` is effectively sampling from the data distribution. But wait - aren't we trying to learn to sample from $p_{\text{data}}$ in the first place? This is a subtlety that we have glossed over thus far. The answer is that *in practice*, `sample_conditioning_variable` would return points from a finite *training set*, which is formally assumed to have been sampled IID from the true distribution $z \sim p_{\text{data}}$.

# %% [markdown]
# ### Problem 2.4: The Conditional Score

# %% [markdown]
# As in lecture may now visualize the conditional trajectories corresponding to the SDE $$d X_t = \left[u_t(X_t|z) + \frac{1}{2}\sigma^2 \nabla_x \log p_t(X_t|z) \right]dt + \sigma\, dW_t, \quad \quad X_0 = x_0 \sim p_{\text{simple}},$$
# obtained by adding *Langevin dynamics* to the original ODE.

# %% [markdown]
# **Your work**: Implement the class method `conditional_score` to compute the conditional distribution $\nabla_x \log p_t(x|z)$, which we compute to be
# $$\nabla_x \log p_t(x|z) = \nabla_x N(x;\alpha_t z,\beta_t^2 I_d) = \frac{\alpha_t z - x}{\beta_t^2}.$$
# To check for correctness, use the next two cells to verify that samples from the conditional SDE match the samples drawn analytically from the conditional probability path.

# %% [markdown]
# **Note**: You may notice that strange things happen for large (or even not-so-large) values of $\sigma$. Plugging in $$\nabla_x \log p_t(x|z) = \frac{\alpha_t z - x}{\beta_t^2}$$ into $$d X_t = \left[u_t(X_t|z) + \frac{1}{2}\sigma^2 \nabla_x \log p_t(X_t|z) \right]dt + \sigma\, dW_t$$ yields
# $$d X_t = \left[u_t(X_t|z) + \frac{1}{2}\sigma^2 \left(\frac{\alpha_t z - X_t}{\beta_t^2}\right) \right]dt + \sigma\, dW_t.$$
# When $t \to 1$, $\beta_t \to 0$, so that the second term of the drift explodes (and this explosion scales quadratically with $\sigma$). With a finite number of simulation steps, we cannot accurately simulate this explosion and thus encounter numerical issues. In practice, this is usually circumvented by setting e.g., $\sigma_t = \beta_t$, so that the exploding affect is canceled out by a gradually decreasing noise level.

# %%
# Run me for Problem 2.3!
function gaussian_conditional_score(x, z, t)
    alpha_t = alpha(t)
    beta_t = beta(t)
    return (z * alpha_t - x) / beta_t^2
end

let
    #######################
    # Change these values #
    #######################
    num_samples = 1000
    num_timesteps = 1000
    num_marginals = 3
    σ = 2.5
    #
    p_data = symmetric_2D(5, PARAMS.target_std, PARAMS.target_scale)
    z = rand(p_data) # Sample conditioning variable z
    ts = range(0.0, 1.0, 7)
    nbin = 200
    scale = PARAMS.scale
    fig = Figure()
    ax = Axis(
        fig[1, 1];
        title = "Gaussian Conditional Probability Path",
        aspect = DataAspect(),
    )
    densityheatmap!(ax; scale, nbin, distribution = p_simple)
    densityheatmap!(
        ax;
        scale,
        nbin,
        distribution = p_data,
        colormap = :reds,
        lowclip = RGBAf(0.0, 0.0, 0.0, 0.0),
    )
    # Plot conditional probability path at each intermediate t
    num_samples = 1000
    x0 = rand(p_simple, num_samples) # Sample from p_simple
    for t in ts
        a, b = alpha(t), beta(t)
        samples = @. a * z + b * x0
        scatter!(
            ax,
            samples[1, :],
            samples[2, :];
            alpha = 0.25,
            markersize = 8,
            label = "t = $(round(t; sigdigits = 1))",
        )
    end
    scatter!(ax, z...; marker = :star5, markersize = 20, color = :green)
    axislegend(ax)
    ax = Axis(fig[1, 2]; title = "Trajectories of conditional ODE", aspect = DataAspect())
    densityheatmap!(ax; scale, nbin, distribution = p_simple)
    densityheatmap!(
        ax;
        scale,
        nbin,
        distribution = p_data,
        colormap = :reds,
        lowclip = RGBAf(0.0, 0.0, 0.0, 0.0),
    )
    for i = 1:15
        x0 = rand(p_simple)
        t = range(0.0, 1.0, num_timesteps)
        drift = (x, t) -> gaussian_conditional_vector_field(x, z, t)
        diff = (x, t) -> σ
        step = (x, t, h) -> euler_maruyama_step(drift, diff, x, t, h)
        x = simulate_with_trajectory(step, x0, t)
        lines!(ax, x[1, :], x[2, :]; color = :black)
    end
    scatter!(ax, z...; marker = :star5, markersize = 20, color = :green)
    fig
end

# %% [markdown]
# # Part 3: Flow Matching and Score Matching with Gaussian Conditional Probability Paths
#

# %% [markdown]
# ### Problem 3.1 Flow Matching with Gaussian Conditional Probability Paths

# %% [markdown]
# Recall now that from lecture that our goal is to learn the *marginal vector field* $u_t(x)$ given by $$u_t^{\text{ref}}(x) = \mathbb{E}_{z \sim p_t(z|x)}\left[u_t^{\text{ref}}(x|z)\right].$$
# Unfortunately, we don't actually know what $u_t^{\text{ref}}(x)$ is! We will thus approximate $u_t^{\text{ref}}(x)$ as a neural network $u_t^{\theta}(x)$, and exploit the identity $$ u_t^{\text{ref}}(x) = \text{argmin}_{u_t(x)} \,\,\mathbb{E}_{z \sim p_t(z|x)} \left[\lVert u_t(x) - u_t^{\text{ref}}(x|z)\rVert^2\right]$$ to obtain the **conditional flow matching objective**
# $$ \mathcal{L}_{\text{CFM}}(\theta) = \,\,\mathbb{E}_{z \sim p(z), x \sim p_t(x|z)} \left[\lVert u_t^{\theta}(x) - u_t^{\text{ref}}(x|z)\rVert^2\right].$$
# To model $u_t^{\theta}(x)$, we'll use a simple MLP. This network will take in both $x$ and $t$, and will return the learned vector field $u_t^{\theta}(x)$.

# %% [markdown]
# Let's first define a general-purpose class `Trainer` to keep things tidy as we start training.

# %% [markdown]
# **Your work**: Fill in `ConditionalFlowMatchingTrainer.get_train_loss` below. This function should implement the conditional flow matching objective $$\mathcal{L}_{\text{CFM}}(\theta) = \,\,\mathbb{E}_{\textcolor{blue}{t \in \mathcal{U}[0,1), z \sim p(z), x \sim p_t(x|z)}} \textcolor{green}{\lVert u_t^{\theta}(x) - u_t^{\text{ref}}(x|z)\rVert^2}$$
# using a Monte-Carlo estimate of the form
# $$\frac{1}{N}\sum_{i=1}^N \textcolor{green}{\lVert u_{t_i}^{\theta}(x_i) - u_{t_i}^{\text{ref}}(x_i|z_i)\rVert^2}, \quad \quad \quad \forall i\in[1, \dots, N]: \textcolor{blue}{\,z_i \sim p_{\text{data}},\, t_i \sim \mathcal{U}[0,1),\, x_i \sim p_t(\cdot | z_i)}.$$
# Here, $N$ is our *batch size*.
#
#
# **Hint 1**: For sampling:
# - You can sample `batch_size` points $z$ from $p_{\text{data}}$ using `self.path.p_data.sample(batch_size)`.
# - You can sample `batch_size` values of `t` using `torch.rand(batch_size, 1)`.
# - You can sample `batch_size` points from `p_t(x|z)` using `self.path.sample_conditional_path(z,t)`.
#
# **Hint 2**: For the loss function:
# - You can access $u_t^{\theta}(x)$ using `self.model(x,t)`.
# - You can access $u_t^{\text{ref}}(x|z)$ using `self.path.conditional_vector_field(x,z,t)`.

# %%

# SiLU activation function
silu(x) = x / (1 + exp(-x))
let
    x = range(-7, 7, 500)
    y = silu.(x)
    lines(x, y)
end

# Construct conditional probability path
p_simple = isotropic(; dim = 2, std = 1.0)
p_data = symmetric_2D(5, PARAMS.target_std, PARAMS.target_scale)

# %% [markdown]
# Now let's train! This may take about a minute... **Remember, the loss should converge, but not to zero!**

flowmodel = let
    net = Chain(
        Dense(3, 64, silu),
        Dense(64, 64, silu),
        Dense(64, 64, silu),
        Dense(64, 64, silu),
        Dense(64, 2),
    )
    rng = Random.default_rng()
    θ, state = Lux.setup(rng, net)
    θ = adapt(Array{Float64}, θ)
    # θ = ComponentArray(θ) .|> Float64
    model(x, t, θ) = first(net(vcat(x, t), θ, state))
    opt_state = Optimisers.setup(Adam(1e-3), θ)
    nepoch = 1000
    nsample = 1000
    loss = MSELoss()
    for i = 1:nepoch
        z = rand(p_data, nsample) # 2 x nsample
        x0 = rand(p_simple, nsample) # 2 x nsample
        t = rand(1, nsample) # 1 x nsample
        x = @. alpha(t) * z + beta(t) * x0 # 2 x nsample
        u = gaussian_conditional_vector_field.(x, z, t) # 2 x nsample
        l, g = withgradient(θ) do θ
            umod = model(x, t, θ) # 2 x nsample
            loss(umod, u)
        end
        gθ = g[1]
        opt_state, θ = Optimisers.update!(opt_state, θ, gθ)
        i % 5 == 0 && @info "i = $i, loss = $l"
    end
    (x, t) -> model(x, t, θ)
end

# %% [markdown]
# Is our model any good? Let's visualize? First, we need to wrap our learned vector field in an subclass of `ODE` so that we can simulate it using our `Simulator` class.

let
    num_timesteps = 1000
    nbin = 200
    scale = PARAMS.scale
    fig = Figure()
    ax = Axis(
        fig[1, 1];
        title = "Ground-truth marginal probability path",
        aspect = DataAspect(),
    )
    # Plot conditional probability path at each intermediate t
    num_samples = 1000
    x0 = rand(p_simple, num_samples) # Sample from p_simple
    z = rand(p_data, num_samples) # Sample conditioning variable z
    ts = range(0.0, 1.0, 4)
    for t in ts
        a, b = alpha(t), beta(t)
        samples = @. a * z + b * x0
        scatter!(
            ax,
            samples[1, :],
            samples[2, :];
            # alpha = 0.25,
            markersize = 8,
            label = "t = $(round(t; sigdigits = 1))",
        )
    end
    axislegend(ax)
    ax = Axis(
        fig[1, 2];
        title = "Trajectories of learned marginal ODE",
        aspect = DataAspect(),
    )
    densityheatmap!(ax; scale, nbin, distribution = p_simple)
    densityheatmap!(
        ax;
        scale,
        nbin,
        distribution = p_data,
        colormap = :reds,
        lowclip = RGBAf(0.0, 0.0, 0.0, 0.0),
    )
    for i = 1:100
        x0 = rand(p_simple)
        t = range(0.0, 1.0, num_timesteps)
        drift = (x, t) -> flowmodel(x, t)
        step = (x, t, h) -> euler_step(drift, x, t, h)
        x = simulate_with_trajectory(step, x0, t)
        lines!(ax, x[1, :], x[2, :]; color = :black)
    end
    fig
end

# %% [markdown]
# ### Problem 3.2: Score Matching with Gaussian Conditional Probability Paths

# %% [markdown]
# We have thus far used flow matching to train a model $u_t^{\theta}(x) \approx u_t^{\text{ref}}$ so that $$d X_t = u_t^{\theta}(X_t) dt $$ approximately passes through the desired marginal probability path $p_t(x)$. Now recall from lecture that we may augment the reference marginal vector field $u_t^{\text{ref}}(x)$ with *Langevin dynamics* to add stochasticity while preserving the marginals, viz., $$dX_t = \left[u_t^{\text{ref}}(x) + \frac{1}{2}\sigma^2 \nabla \log p_t(x)\right] dt + \sigma d W_t.$$
# Substituting our learned approximation $u_t^{\theta}(x) \approx u_t^{\text{ref}}$ therefore yields
# $$dX_t = \left[u_t^{\theta}(x) + \frac{1}{2}\sigma^2 \nabla \log p_t(x)\right] dt + \sigma d W_t.$$
# There's just one issue, what's the marginal score $\nabla \log p_t(x)$? In Question 2.3, we computed the conditional score $\nabla \log p_t(x|z)$ of the Gaussian probability path. In the same way that we learned an approximation $u_t^{\theta}(x) \approx u_t^{\text{ref}}$, we'd like to be able to learn a similar approximation $s_t^{\theta}(x) \approx \nabla \log p_t(x)$. Recall from lecture the identity $$\nabla \log p_t(x) = \mathbb{E}_{z \sim p_t(z|x)}\left[\nabla \log p_t(x|z) \right].$$ It then immediately follows that
# $$\nabla \log p_t(x) = \text{argmin}_{s_t(x)} \,\,\mathbb{E}_{z \sim p(z), x \sim p_t(x|z)} \left[\lVert s_t(x) - \nabla \log p_t(x|z)\rVert^2\right].$$
# We thus obtain the **conditional score matching** loss
# $$\mathcal{L}_{\text{CSM}}(\theta) \triangleq \mathbb{E}_{t \sim \mathcal{U}[0,1), z \sim p(z), x \sim p_t(x|z)} \left[\lVert s_t^{\theta}(x) - \nabla \log p_t(x|z)\rVert^2\right].$$
# Here, we will parameterize $s_t^{\theta}(x): \mathbb{R}^2 \to \mathbb{R}^2$ as a simple MLP, just like $u_t^{\theta}(x)$.

# %% [markdown]
# **Your job**: Fill in method `ConditionalScoreMatchingTrainer.get_train_loss` to implement the conditional score matching loss $\mathcal{L}_{\text{CSM}}(\theta)$.
#
# **Hint:** Remember to re-use your implementation of `GaussianConditionalProbabilityPath.conditional_score`!

scoremodel = let
    net = Chain(
        Dense(3, 64, silu),
        Dense(64, 64, silu),
        Dense(64, 64, silu),
        Dense(64, 64, silu),
        Dense(64, 2),
    )
    rng = Random.default_rng()
    θ, state = Lux.setup(rng, net)
    θ = adapt(Array{Float64}, θ)
    # θ = ComponentArray(θ) .|> Float64
    model(x, t, θ) = first(net(vcat(x, t), θ, state))
    opt_state = Optimisers.setup(Adam(1e-3), θ)
    nepoch = 1000
    nsample = 1000
    loss = MSELoss()
    for i = 1:nepoch
        z = rand(p_data, nsample) # 2 x nsample
        x0 = rand(p_simple, nsample) # 2 x nsample
        t = rand(1, nsample) # 1 x nsample
        x = @. alpha(t) * z + beta(t) * x0 # 2 x nsample
        s = gaussian_conditional_score.(x, z, t) # 2 x nsample
        l, g = withgradient(θ) do θ
            smod = model(x, t, θ) # 2 x nsample
            loss(smod, s)
        end
        gθ = g[1]
        opt_state, θ = Optimisers.update!(opt_state, θ, gθ)
        i % 5 == 0 && @info "i = $i, loss = $l"
    end
    (x, t) -> model(x, t, θ)
end

# %% [markdown]
# Now let's visualize our work! Before we do however, we'll need to wrap our learned our flow model and score model in an instance of `SDE` so that we can integrate it using our `EulerMaruyamaIntegrator` class. This new class, `LangevinFlowSDE` will correspond to the dynamics $$dX_t = \left[u_t^{\theta}(x) + \frac{1}{2}\sigma^2 s_t^{\theta}(x)\right] dt + \sigma d W_t.$$

# %%
let
    σ = 2.0 # Don't set sigma too large or you'll get numerical issues!
    num_timesteps = 300
    nbin = 200
    scale = PARAMS.scale
    fig = Figure()
    ax = Axis(
        fig[1, 1];
        title = "Ground-truth marginal probability path",
        aspect = DataAspect(),
    )
    # Plot conditional probability path at each intermediate t
    num_samples = 1000
    x0 = rand(p_simple, num_samples) # Sample from p_simple
    z = rand(p_data, num_samples) # Sample conditioning variable z
    ts = range(0.0, 1.0, 4)
    for t in ts
        a, b = alpha(t), beta(t)
        samples = @. a * z + b * x0
        scatter!(
            ax,
            samples[1, :],
            samples[2, :];
            # alpha = 0.25,
            markersize = 8,
            label = "t = $(round(t; sigdigits = 1))",
        )
    end
    axislegend(ax)
    ax = Axis(
        fig[1, 2];
        title = "Trajectories of learned marginal SDE",
        aspect = DataAspect(),
    )
    densityheatmap!(ax; scale, nbin, distribution = p_simple)
    densityheatmap!(
        ax;
        scale,
        nbin,
        distribution = p_data,
        colormap = :reds,
        lowclip = RGBAf(0.0, 0.0, 0.0, 0.0),
    )
    for i = 1:50
        x0 = rand(p_simple)
        t = range(0.0, 1.0, num_timesteps)
        drift = (x, t) -> flowmodel(x, t) + σ^2 / 2 * scoremodel(x, t)
        diff = (x, t) -> σ
        step = (x, t, h) -> euler_maruyama_step(drift, diff, x, t, h)
        x = simulate_with_trajectory(step, x0, t)
        lines!(ax, x[1, :], x[2, :]; linewidth = 0.5, color = :black)
    end
    fig
end

# %% [markdown]
# ### Question 3.3: Deriving the Marginal Score from the Marginal Flow
# Recall from the notes and the lecture that for Gaussian probability paths $$u_t^{\text{ref}}(x) = a_tx + b_t\nabla \log p_t^{\text{ref}}(x).$$
#
# where $(a_t, b_t) = \left(\frac{\dot{\alpha}_t}{\alpha_t}, \beta_t^2 \frac{\dot{\alpha}_t}{\alpha_t} - \dot{\beta}_t \beta_t\right)$. Rearranging yields $$\nabla \log p_t^{\text{ref}}(x) = \frac{u_t^{\text{ref}}(x) - a_tx}{b_t}.$$
#
# Therefore, we may instead exploit the fact that we have already trained $u_t^{\theta}(x)$, to parameterize $s_t^{\theta}(x)$ via
# $$\tilde{s}_t^{\theta}(x) = \frac{u_t^{\theta}(x) - a_tx}{b_t} = \frac{\alpha_t u_t^{\theta}(x) - \dot{\alpha}_t x}{\beta_t^2 \dot{\alpha}_t - \alpha_t \dot{\beta}_t \beta_t},$$
# so long as $\beta_t^2 \dot{\alpha}_t - \alpha_t \dot{\beta}_t \beta_t \neq 0$ (which is true for $t \in [0,1)$ by monotonicity). Here, we differentiate $\tilde{s}_t^{\theta}(x)$ paramterized via $u_t^{\theta}(x)$ from $s_t^{\theta}(x)$ learned indepedently using score matching. Plugging in $\alpha_t = t$ and $\beta_t = \sqrt{1-t}$, we find that $$\beta_t^2 \dot{\alpha}_t - \alpha_t \dot{\beta}_t \beta_t = \begin{cases} 1 - \frac{t}{2} & \text{if}\,\,t\in [0,1)\\0 & \text{if}\,\,{t=1}. \end{cases}.$$ In the following visualization, we'll circumvent the issue at $t=1.0$ by taking $t=1 - \varepsilon$ in place of $t=1$, for small $\varepsilon \approx 0$.

# %% [markdown]
# **Your job**: Implement $\tilde{s}_t^{\theta}(x)$ by filling in the body of `ScoreFromVectorField.forward` below. The next several cells generate a visualization comparing the flow-parameterized score $\tilde{s}_t^{\theta}(x)$ to our independently learned score $s_t^{\theta}(x)$. You can check that your implementation is correct by making sure that the visualizations match.

# %%
function score_from_vectorfield(u, x, t)
    alpha_t = alpha(t)
    beta_t = beta(t)
    dt_alpha_t = ForwardDiff.derivative(alpha, t)
    dt_beta_t = ForwardDiff.derivative(beta, t)
    num = alpha_t * u - dt_alpha_t * x
    den = beta_t^2 * dt_alpha_t - alpha_t * dt_beta_t * beta_t
    num / den
end

# %% [markdown]
# Now, let's compare our learned marginal score $s_t^{\theta}(x)$ (an instance of `MLPScore`) to our flow-parameterized score (an instance of `ScoreFromVectorField`). We'll do so by plotting the vector fields across time and space.

# %%
###############################
# Plot score fields over time #
###############################
let
    nx = 20
    nbin = 200
    nmarginal = 3
    scale = PARAMS.scale
    fig = Figure(; size = (nmarginal * 300, 600))
    for i = 1:nmarginal
        t = range(0.0, 1.0 - 1e-4, nmarginal)[i]
        tround = round(t; sigdigits = 2)
        for j = 1:2
            title = j == 1 ? "Score learned directly" : "Score from learned flow"
            title = "$title, t = $tround"
            ax = Axis(
                fig[j, i];
                title,
                aspect = DataAspect(),
                xticksvisible = false,
                yticksvisible = false,
                xticklabelsvisible = false,
                yticklabelsvisible = false,
            )
            # Plot the marginal probability path
            densityheatmap!(ax; scale, nbin, distribution = p_simple)
            densityheatmap!(
                ax;
                scale,
                nbin,
                distribution = p_data,
                colormap = :reds,
                lowclip = RGBAf(0.0, 0.0, 0.0, 0.0),
            )
            # Plot the flow-parameterized score field
            x = range(-scale, scale, nx)
            xy = broadcast(1:2, reshape(x, 1, :), reshape(x, 1, 1, :)) do i, x, y
                i == 1 ? x : y
            end
            xy = reshape(xy, 2, :)
            tt = fill(t, 1, size(xy, 2))
            s = if j == 1
                scoremodel(xy, tt)
            else
                u = flowmodel(xy, tt)
                score_from_vectorfield.(u, xy, t)
            end
            arrows2d!(
                ax,
                xy[1, :],
                xy[2, :],
                s[1, :],
                s[2, :];
                color = :black,
                lengthscale = 2e-1,
            )
        end
    end
    fig
end

# %% [markdown]
# # Part 4: Flow Matching Between Arbitrary Distributions with a Linear Probability Path
# In this section, we will consider an alterntive conditional probability path - the **linear conditional probability path** - which can be constructed as follows. Given a source distribution $p_{\text{simple}}$ and a data distribution $p_{\text{data}}$, for a fixed $z$ we may consider the *interpolant* $$X_t = (1-t) X_0 + tz$$
# where $X_0 \sim p_{\text{simple}}$ is a random variable. We may then define $p_t(x|z)$ so that $X_t \sim p_t(x|z)$.  Then it is apparent that $p_0(x|z) = p_{\text{simple}}(x)$ and $p_1(x| z)= \delta_z(x)$. It is also not difficult to show that the conditional vector field is given by $u_t^{\text{ref}}(x) = \frac{z - x}{1-t}$ for $t \in [0,1)$. We make two observations about the linear conditional probability path: First, unlike in the Gaussian probability path, we do not have a closed form for the conditional score $\nabla \log p_t(x|z)$. Second, there is no constraint that $p_{\text{simple}}$ be a Gaussian, which we will exploit in Problem 4.3 to construct flows between arbitrary choices of $p_{\text{simple}}$ and $p_{\text{data}}$. First, let's examine some more complicated choices of $p_{\text{data}}$.

struct Circles{T} <: Sampleable{Multivariate,Continuous}
    scale::T
    factor::T
    noise::T
end

struct Moons{T} <: Sampleable{Multivariate,Continuous}
    scale::T
    noise::T
end

struct Checkerboard{T} <: Sampleable{Multivariate,Continuous}
    scale::T
    n::Int
end

# Distribution samples are 2D
Base.length(::Circles) = 2
Base.length(::Moons) = 2
Base.length(::Checkerboard) = 2

function Distributions._rand!(rng::AbstractRNG, c::Circles, u::AbstractVector{T}) where {T}
    phase = rand(rng)
    x = cospi(2 * phase)
    y = sinpi(2 * phase)
    isouter = rand(rng, (true, false))
    factor = isouter ? one(T) : c.factor
    u[1] = c.scale * (factor * x + c.noise * randn(rng))
    u[2] = c.scale * (factor * y + c.noise * randn(rng))
end

function Distributions._rand!(rng::AbstractRNG, m::Moons, u::AbstractVector{T}) where {T}
    isupper = rand(rng, (true, false))
    phase = rand(rng)
    if isupper
        x = cospi(phase)
        y = sinpi(phase)
    else
        x = 1 - cospi(phase)
        y = T(1 / 2) - sinpi(phase)
    end
    u[1] = m.scale * (x + m.noise * randn(rng))
    u[2] = m.scale * (y + m.noise * randn(rng))
end

function Distributions._rand!(
    rng::AbstractRNG,
    c::Checkerboard,
    u::AbstractVector{T},
) where {T}
    docontinue = true
    i, j = 0, 1
    while xor(iseven(i), iseven(j))
        i = rand(rng, 1:c.n)
        j = rand(rng, 1:c.n)
    end
    u[1] = 2 * c.scale * ((rand(rng) + i - 1) / c.n - T(1 / 2))
    u[2] = 2 * c.scale * ((rand(rng) + j - 1) / c.n - T(1 / 2))
end

let
    distributions = Circles(5.0, 0.5, 0.05), Moons(3.5, 0.05), Checkerboard(5.0, 4)
    titles = ["Circles", "Moons", "Checkerboard"]
    fig = Figure(; size = (length(distributions) * 300, 400))
    for (i, p) in enumerate(distributions)
        ax = Axis(fig[1, i]; title = titles[i])
        x = rand(p, 20_000)
        hexbin!(ax, x[1, :], x[2, :]; bins = 200)
        # scatter!(ax, x[1, :], x[2, :])
        scale = 7.5
        xlims!(ax, -scale, scale)
        ylims!(ax, -scale, scale)
        ax.aspect = DataAspect()
    end
    fig
end

# %% [markdown]
# ### Problem 4.1: Linear Probability Paths
# Below we define the `LinearConditionalProbabilityPath`. We purposely omit the implementation of `conditional_score` because, as mentioned earlier, there is no nice form for it!

linear_conditional_vector_field(x, z, t) = (z - x) / (1 - t)

# %% [markdown]
# **Your work**: Implement `LinearConditionalProbabilityPath.sample_conditional_path` and `LinearConditionalProbabilityPath.conditional_vector_field`.

# %% [markdown]
# You can sanity check that the implementations are correct by ensuring that they are consistent with one another. The following visualization provides three sequences of graphs:
# 1. The first row shows the conditional probability path, as produced by your implemententation of `sample_conditional_path`.
# 2. The second row shows the conditional probability path, as produced by your implemententation of `conditional_vector_field`.
# 3. The third row shows the marginal probability path, as produced by `sample_marginal_path`.

# %%
let
    num_samples = 100000
    num_timesteps = 100
    num_marginals = 5
    scale = 6.0
    ts = range(0.0, 1.0, num_marginals)
    p_simple = isotropic(1.0)
    p_data = Checkerboard(5.0, 4)
    z = rand(p_data, 1)
    fig = Figure(; size = (num_marginals * 170, 500))
    tode = 0.0
    xode = rand(p_simple, num_samples)
    for (i, t) in enumerate(ts)
        x0 = rand(p_simple, num_samples)
        x = @. (1 - t) * x0 + t * z
        i > 1 && for tode in range(ts[i - 1], ts[i], num_timesteps + 1)[1:end - 1]
            h = 1 / num_timesteps / num_marginals
            drift = linear_conditional_vector_field.(xode, z, tode)
            @. xode += drift * h
        end
        zsamples = rand(p_data, num_samples)
        xmarginal = @. (1 - t) * x0 + t * zsamples
        tround = round(t; sigdigits = 2)
    ylabels = [
    "Conditional (ground-truth)",
    "Conditional (ODE)",
    "Marginal",
    ]
        ax1, ax2, ax3 = ntuple(
            j -> Axis(
                fig[j, i];
                title = "t = $tround",
                titlevisible = j == 1,
                aspect = DataAspect(),
                limits = (-scale, scale, -scale, scale),
                xticksvisible = false,
                yticksvisible = false,
                xticklabelsvisible = false,
                yticklabelsvisible = false,
                ylabel = ylabels[j],
                ylabelvisible = i == 1,
            ),
        3,
        )
        hexbin!(ax1, x[1, :], x[2, :]; bins = 200)
        hexbin!(ax2, xode[1, :], xode[2, :]; bins = 200)
        hexbin!(ax3, xmarginal[1, :], xmarginal[2, :]; bins = 200)
        if i == num_marginals
            scatter!(ax1, z...; marker = :star5, markersize = 20, color = :green)
            scatter!(ax2, z...; marker = :star5, markersize = 20, color = :green)
        end
    end
    fig
end

# %% [markdown]
# ### Part 4.2: Flow Matching with Linear Probability Paths
# Now, let's train a flow matching model using the linear conditional probability path! **Remember, the loss should converge, but not necessarily to zero!**
#
# Notice that in our construction of the linear probability path, there is no need for $p_{\text{simple}}$ to be a Gaussian. Let's try setting it to another distribution!

## p_simple = isotropic(1.0)
p_simple = Circles(5.0, 0.5, 0.05)
p_data = Checkerboard(5.0, 4)

flowmodel = let
    net = Chain(
        Dense(3, 64, silu),
        Dense(64, 64, silu),
        Dense(64, 64, silu),
        Dense(64, 64, silu),
        Dense(64, 2),
    )
    rng = Random.default_rng()
    θ, state = Lux.setup(rng, net)
    θ = adapt(Array{Float64}, θ)
    model(x, t, θ) = first(net(vcat(x, t), θ, state))
    opt_state = Optimisers.setup(Adam(2e-3), θ)
    nepoch = 20000
    nsample = 2000
    loss = MSELoss()
    for i = 1:nepoch
        z = rand(p_data, nsample) # 2 x nsample
        x0 = rand(p_simple, nsample) # 2 x nsample
        t = rand(1, nsample) # 1 x nsample
        x = @. t * z + (1 - t) * x0 # 2 x nsample
        u = linear_conditional_vector_field.(x, z, t) # 2 x nsample
        l, g = withgradient(θ) do θ
            umod = model(x, t, θ) # 2 x nsample
            loss(umod, u)
        end
        gθ = g[1]
        opt_state, θ = Optimisers.update!(opt_state, θ, gθ)
        i % 5 == 0 && @info "i = $i, loss = $l"
    end
    (x, t) -> model(x, t, θ)
end

# %%
let
    num_samples = 50000
    num_timesteps = 20
    num_marginals = 5
    scale = 6.0
    ts = range(0.0, 1.0, num_marginals)
    fig = Figure(; size = (num_marginals * 170, 400))
    tode = 0.0
    x0 = rand(p_simple, num_samples)
    xode = copy(x0)
    for (i, t) in enumerate(ts)
        i > 1 && for (ii, tode) in enumerate(range(ts[i - 1], ts[i], num_timesteps + 1)[1:end - 1])
            h = 1 / num_timesteps / num_marginals
            tt = fill(tode, 1, num_samples)
            drift = flowmodel(xode, tt)
            @. xode += drift * h
            ii % 5 == 0 && @show tode
        end
        zsamples = rand(p_data, num_samples)
        xmarginal = @. (1 - t) * x0 + t * zsamples
        tround = round(t; sigdigits = 2)
        ylabels = ["Ground-truth", "Learned"]
        ax1, ax2 = ntuple(
            j -> Axis(
                fig[j, i];
                title = "t = $tround",
                titlevisible = j == 1,
                aspect = DataAspect(),
                limits = (-scale, scale, -scale, scale),
                xticksvisible = false,
                yticksvisible = false,
                xticklabelsvisible = false,
                yticklabelsvisible = false,
                ylabel = ylabels[j],
                ylabelvisible = i == 1,
            ),
            2,
        )
        hexbin!(ax1, xmarginal[1, :], xmarginal[2, :]; bins = 200)
        hexbin!(ax2, xode[1, :], xode[2, :]; bins = 200)
    end
    fig
end

# %% [markdown]
# **Your job**: Play around with the choice of $p_{\text{simple}}$ and $p_{\text{data}}$. Any observations?
